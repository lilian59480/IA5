{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Dites-le avec des fleurs !"},{"metadata":{},"cell_type":"markdown","source":"## Librairies utiles"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Directive pour afficher les graphiques dans Jupyter\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pandas : librairie de manipulation de données\n# NumPy : librairie de calcul scientifique\n# MatPlotLib : librairie de visualisation et graphiques\n# SeaBorn : librairie de graphiques avancés\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Le dataset des iris"},{"metadata":{},"cell_type":"markdown","source":"Le dataset des iris est prédéfini dans Seaborn :"},{"metadata":{"trusted":true},"cell_type":"code","source":"iris = pd.read_csv(\"../input/iris.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On peut afficher les 10 premières lignes du dataset :"},{"metadata":{"trusted":true},"cell_type":"code","source":"iris.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On a les informations suivantes :\n- longueur du sépale (en cm)\n- largeur du sépale\n- longueur du pétale\n- largeur du pétale\n- espèce : Virginica, Setosa ou Versicolor"},{"metadata":{},"cell_type":"markdown","source":"<img src=\"http://python.astrotech.io/_images/iris-flowers.png\">"},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQM3aH4Q3AplfE1MR3ROAp9Ok35fafmNT59ddXkdEvNdMkT8X6E\">"},{"metadata":{},"cell_type":"markdown","source":"## Données"},{"metadata":{},"cell_type":"markdown","source":"Pour accéder à une colonne avec une notation pointée ou entre crochets :"},{"metadata":{"trusted":true},"cell_type":"code","source":"iris.species\niris['species']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"La méthode *shape* permet de vérifier qu'on a 150 lignes et 5 colonnes"},{"metadata":{"trusted":true},"cell_type":"code","source":"iris.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On peut afficher le type de chaque colonne avec la méthode *info()*"},{"metadata":{"trusted":true},"cell_type":"code","source":"iris.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*describe()* permet d'afficher des statistiques sur le dataset :"},{"metadata":{"trusted":true},"cell_type":"code","source":"iris.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On peut compter le nombre d'éléments de chaque espèce :"},{"metadata":{"trusted":true},"cell_type":"code","source":"iris.species.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Avec une expression booléenne, on peut sélectionner les lignes correspondant à l'espèce Setosa dont les sépales sont plus longs que 5cm :"},{"metadata":{"trusted":true},"cell_type":"code","source":"iris[(iris.species=='setosa') & (iris.sepal_length>5)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On peut aussi prédéfinir des booléens :"},{"metadata":{"trusted":true},"cell_type":"code","source":"setosa = iris.species=='setosa'\nvirginica = iris.species=='virginica'\nversicolor = iris.species=='versicolor'\ngrand_sepale = iris.sepal_length>5","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"et les utiliser pour sélectionner des lignes :"},{"metadata":{"trusted":true},"cell_type":"code","source":"iris[setosa & grand_sepale]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** Pour plus de détails sur Pandas et la manipulation de Dataframes :  \nhttps://github.com/jhroy/tuto-pandas/blob/master/tutoriel.ipynb  \nhttps://www.datacamp.com/community/tutorials/pandas-tutorial-dataframe-python  **"},{"metadata":{},"cell_type":"markdown","source":"## Visualisations"},{"metadata":{},"cell_type":"markdown","source":"Pour tracer un histogramme, il faut définir le nombre de *bins* (barres de l'histogramme)"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(iris.sepal_length, color=\"green\", bins=100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ou seulement pour l'espèce Setosa :"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(iris[setosa].sepal_length, bins=100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pour superposer plusieurs histogrammes, on peut définir une transparence avec le paramètre *alpha*"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,6))\nplt.hist(iris[setosa].petal_length, color='blue', bins=50, alpha=0.3)\nplt.hist(iris[versicolor].petal_length, color='red', bins=50, alpha=0.3)\nplt.hist(iris[virginica].petal_length, color='green', bins=50, alpha=0.3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*seaborn* (en abrégé *sns*) permet de réaliser des graphiques facilement :"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(\"petal_length\", data=iris, hue=\"species\", kind=\"count\", height=8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On peut également tracer estimation de la densité de probabilité d'une variable :"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.kdeplot(iris[setosa].sepal_length)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ou en superposant distribution et histogramme :"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(iris[setosa].sepal_length)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*jointplot* permet de visualiser dans un plan les distributions d'un couple de paramètres :"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.jointplot(\"petal_length\", \"petal_width\", iris, kind='kde');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Les **diagrammes en boîte** (ou **boîtes à moustaches** ou **box plot**) résument quelques caractéristiques de position du caractère étudié (médiane, quartiles, minimum, maximum ou déciles). Ce diagramme est utilisé principalement pour comparer un même caractère dans deux populations de tailles différentes. Il s'agit de tracer un rectangle allant du premier quartile au troisième quartile et coupé par la médiane. On ajoute alors des segments aux extrémités menant jusqu'aux valeurs extrêmes.  \nPar exemple pour la répartion des espèces selon la longueur du sépale :"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=\"species\", y=\"sepal_length\", data=iris)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Les **violins plots** sont similaires aux box plots, excepté qu’ils permettent de montrer la courbe de densité de probabilité des différentes valeurs. Typiquement, les violins plots présentent un marqueur pour la médiane des données et l’écart interquartile, comme dans un box plot standard."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.violinplot(x=\"species\", y=\"petal_length\", data=iris)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*FacetGrid* permet de superposer des graphiques selon une ou plusieurs caractéristiques. On crée une structure avec *FacetGrid*, et on trace ensuite les graphiques avec *map*"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = sns.FacetGrid(iris, hue=\"species\", aspect=3, palette=\"autumn\") # aspect=3 permet d'allonger le graphique\nfig.map(sns.kdeplot, \"petal_width\", shade=True)\nfig.add_legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On veut tracer un nuage de points selon la longueur du pétale et la longueur du pétale, en différentiant les espèces :"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lmplot(x=\"sepal_length\", y=\"petal_length\", data=iris, fit_reg=False, hue='species')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*pairplot* affiche les nuages de points associés à tous les couples de paramètres (attention, le temps de calcul peut être long) :"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(iris[iris.species!='setosa'], hue=\"species\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Machine learning"},{"metadata":{},"cell_type":"markdown","source":"La plupart des algorithmes ont besoin de données numériques, et n'acceptent pas les chaînes de caractères. On va \"mapper\" les noms d'espèces vers des nombres, en créant une nouvelle colonne 'classe' :"},{"metadata":{"trusted":true},"cell_type":"code","source":"iris['classe'] = iris.species.map({\"setosa\":0, \"versicolor\":1, \"virginica\":2})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"iris.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On utilise *drop* pour supprimer la colone *species* (*axis=1* indique qu'on opère sur les colonnes et non sur les lignes):"},{"metadata":{"trusted":true},"cell_type":"code","source":"iris = iris.drop(['species'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"iris.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On sépare le dataset en deux parties :\n- un ensemble d'apprentissage (entre 70% et 90% des données), qui va permettre d'entraîner le modèle\n- un ensemble de test (entre 10% et 30% des données), qui va permettre d'estimer la pertinence de la prédiction"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train = iris.sample(frac=0.8)          # 80% des données avec frac=0.8\ndata_test = iris.drop(data_train.index)     # le reste des données pour le test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On sépare les données d'apprentissage (*X_train*) et la cible (*y_train*, la colonnes des données *classe*)"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = data_train.drop(['classe'], axis=1)\ny_train = data_train.classe\nX_test = data_test.drop(['classe'], axis=1)\ny_test = data_test.classe","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Régression logistique"},{"metadata":{},"cell_type":"markdown","source":"On veut prédire une variable aléatoire $Y$ à partir d'un vecteur de variables explicatives $X=(X_1,...,X_n)$\nOn \n"},{"metadata":{},"cell_type":"markdown","source":"La fonction logistique $\\frac{e^{x}}{1+e^{x}}$ varie entre $-\\infty$ et $+\\infty$ pour $x$ variant entre $0$ et $1$.  \nElle est souvent utilisée pour \"mapper\" une probabilité et un espace réel"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(9,9))\n\nlogistique = lambda x: np.exp(x)/(1+np.exp(x))   \n\nx_range = np.linspace(-10,10,50)       \ny_values = logistique(x_range)\n\nplt.plot(x_range, y_values, color=\"red\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"La régression logistique consiste à trouver une fonction linéaire C(X) qui permette d'estimer la probabilité de $Y=1$ connaissant $X$ :\n$$p(Y=1|X) = \\frac{e^{C(X)}}{1+e^{C(X)}}$$"},{"metadata":{},"cell_type":"markdown","source":"Autrement dit, cela revient à trouver une séparation linéaire des caractéristiques qui minimise un critère d'erreur."},{"metadata":{},"cell_type":"markdown","source":"Pour plus de détails, cf par exemple :  \nhttp://eric.univ-lyon2.fr/~ricco/cours/cours/pratique_regression_logistique.pdf"},{"metadata":{},"cell_type":"markdown","source":"On veut maintenant prédire l'espèce à partir de toutes les caractéristiques, et évaluer la qualité de cette prédiction en utilisant la régression logistique définie dans la librairie *sklearn* :"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On entraîne le modèle de régression logistique avec *fit* :"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression()\nlr.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On peut prédire les valeurs sur l'ensemble de test avec le modèle entraîné :"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_lr = lr.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.array(y_test))\nprint(y_lr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Score et matrice de confusion"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"La mesure de pertinence compte le nombre de fois où l'algorithme a fait une bonne prédiction (en pourcentage) :"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_score = accuracy_score(y_test, y_lr)\nprint(lr_score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Une mesure plus fine consiste à compter le nombre de **faux positif** (valeur prédite 1 et réelle 0) et de **vrai négatif** (valeur prédite 0 et réelle 1). On utilise une **matrice de confusion** :"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Matrice de confusion\ncm = confusion_matrix(y_test, y_lr)\nprint(cm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://i.stack.imgur.com/gKyb9.png\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Arbres de décision"},{"metadata":{},"cell_type":"markdown","source":"Un arbre de décision permet de faire à chaque étape un choix entre deux possibilités, pour arriver à une réponse sur les feuilles (cf. Akinator)  \n<img src=\"https://scontent-cdg2-1.xx.fbcdn.net/v/t1.0-9/580017_474689062557542_1211572618_n.jpg?_nc_cat=100&oh=d87e6f9628499f5c872d0a375ab5c477&oe=5C28C2B9\">"},{"metadata":{},"cell_type":"markdown","source":"Pour construire un arbre de décision à partir d'un ensemble d'apprentissage, on va choisir une variable qui sépare l'ensemble en deux parties les plus distinctes en fonction d'un critère. Sur les iris par exemple, on peut utiliser la largeur du pétale pour séparer l'espèce Setosa des autres."},{"metadata":{},"cell_type":"markdown","source":"L'indice *GINI* mesure avec quelle fréquence un élément aléatoire de l'ensemble serait mal classé si son étiquette était sélectionnée aléatoirement depuis la distribution des étiquettes dans le sous-ensemble."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import tree\ndtc = tree.DecisionTreeClassifier()\ndtc.fit(X_train,y_train)\ny_dtc = dtc.predict(X_test)\nprint(accuracy_score(y_test, y_dtc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"L'arbre de décision :\n<img src=\"http://cedric.cnam.fr/vertigo/Cours/ml2/_images/tparbresiris.svg\">"},{"metadata":{},"cell_type":"markdown","source":"On peut modifier certains paramètres :  Le paramètre *max_depth* est un seuil sur la profondeur maximale de l’arbre. Le paramètre *min_samples_leaf* donne le nombre minimal d’échantillons dans un noeud feuille."},{"metadata":{"trusted":true},"cell_type":"code","source":"dtc1 = tree.DecisionTreeClassifier(max_depth = 3, min_samples_leaf = 20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On obtient un arbre un peu différent :\n<img src=\"http://cedric.cnam.fr/vertigo/Cours/ml2/_images/tparbresiris1.svg\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"dtc1.fit(X_train,y_train)\ny_dtc1 = dtc1.predict(X_test)\nprint(accuracy_score(y_test, y_dtc1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pour plus de détails sur les arbres de décision :  \nhttps://zestedesavoir.com/tutoriels/962/les-arbres-de-decisions/comprendre-le-concept/#1-les-origines  \nhttp://cedric.cnam.fr/vertigo/Cours/ml2/tpArbresDecision.html  \nhttp://perso.mines-paristech.fr/fabien.moutarde/ES_MachineLearning/Slides/coursFM_AD-RF.pdf  "},{"metadata":{},"cell_type":"markdown","source":"## Random forests"},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://infinitescript.com/wordpress/wp-content/uploads/2016/08/Random-Forest-Example.jpg\">"},{"metadata":{},"cell_type":"markdown","source":"cf par exemple :  \nhttps://fr.wikipedia.org/wiki/For%C3%AAt_d%27arbres_d%C3%A9cisionnels  \nhttps://www.biostars.org/p/86981/  \nhttps://infinitescript.com/2016/08/random-forest/"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import ensemble\nrf = ensemble.RandomForestClassifier()\nrf.fit(X_train, y_train)\ny_rf = rf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_score = accuracy_score(y_test, y_rf)\nprint(rf_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_test, y_rf)\nprint(cm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Importance des caractéristiques"},{"metadata":{},"cell_type":"markdown","source":"L'attribut *feature_importances_* renvoie un tableau du poids de chaque caractéristique dans la décision :"},{"metadata":{"trusted":true},"cell_type":"code","source":"importances = rf.feature_importances_\nprint(importances)\nindices = np.argsort(importances)\nprint(indices)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On peut visualiser ces degrés d'importance avec un graphique à barres par exemple :"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,5))\nplt.barh(range(len(indices)), importances[indices], color='r', align='center')\nplt.yticks(range(len(indices)), iris.columns[indices])\nplt.title('Importance des caracteristiques')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Support Vector Machines"},{"metadata":{},"cell_type":"markdown","source":"cf par exemple :  \nhttps://medium.com/machine-learning-101/chapter-2-svm-support-vector-machine-theory-f0812effc72  \nhttp://scikit-learn.org/stable/modules/svm.html  \nhttp://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html  \nhttps://www.svm-tutorial.com/  \nhttp://www.grappa.univ-lille3.fr/~ppreux/papiers/man.pdf\n<img src = \"http://scikit-learn.org/stable/_images/sphx_glr_plot_iris_001.png\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import svm\nsvc = svm.SVC()\nsvc.fit(X_train, y_train)\ny_svc = svc.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svc_score = accuracy_score(y_test, y_svc)\nprint(svc_score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Clustering"},{"metadata":{},"cell_type":"markdown","source":"Les problèmes de *segmentation* sont assez courants : par exemple, on souhaite répartir des clients en plusieurs catégories, pour différencier des envois publicitaires.  \nOn a un problème *non supervisé* : on suppose qu'on ne connaît pas la catégorie des données.  \nLa méthode la plus usuelle est celle des *k plus proches voisins*. On veut décomposer les données en *k* classes. On chosit aléatoirement *k* points dans l'espace des données, et affecte chaque donnée au groupe *i* si cette donnée est plus proche du point *i*. On recalcule alors les *k* barycentres de chaque groupe obtenu, et on recommence tant que les barycentres changent.  \ncf par exemple :  \nhttps://fr.wikipedia.org/wiki/Recherche_des_plus_proches_voisins  \nhttp://www.grappa.univ-lille3.fr/polys/fouille/sortie005.html  \nIl existe de nombreuses autres méthodes de clustering :  \nhttp://scikit-learn.org/stable/modules/clustering.html  \nhttps://hdbscan.readthedocs.io/en/latest/comparing_clustering_algorithms.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On \"oublie\" l'espèce des iris :"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_iris = iris.drop(['classe'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On utilise donc la méthode des k plus proches voisins (http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) :"},{"metadata":{"trusted":true},"cell_type":"code","source":"km = KMeans(n_clusters=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"km.fit(X_iris)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"iris_labels = km.predict(X_iris)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On peut vérifier qu'on retrouve (globalement) la décomposition en espèces (évidemment les numéros ne correspondent pas) :"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"iris_labels, np.array(iris.classe)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On peut visualiser les clusters obtenus (ici on choisit en x la longueur de pétale, en y la largeur de pétale et en diamètre du point la longueur de sépale) :"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(X_iris.petal_length, X_iris.petal_width, c = iris_labels, cmap=\"Set3\", s=(iris.sepal_length**2)*5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Et on peut comparer aux espèces (les couleurs ne correspondent pas) :"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(X_iris.petal_length, X_iris.petal_width, c = iris.classe, cmap=\"Set3\", s=(iris.sepal_length**2)*5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On voit qu'on a bien retrouvé l'espèce Setosa, mais que l'on confond plus fréquemment Versicolor et Virginica"},{"metadata":{},"cell_type":"markdown","source":"La **classification ascendante hiérarchique** (CAH) est une méthode de classification itérative dont le principe est simple.  \n- On commence par calculer la dissimilarité entre les N objets.  \n- Puis on regroupe les deux objets dont le regroupement minimise un critère d'agrégation donné, créant ainsi une classe comprenant ces deux objets.  \n- On calcule ensuite la dissimilarité entre cette classe et les N-2 autres objets en utilisant le critère d'agrégation. Puis on regroupe les deux objets ou classes d'objets dont le regroupement minimise le critère d'agrégation.\nOn continue ainsi jusqu'à ce que tous les objets soient regroupés.\n\nCes regroupements successifs produisent un arbre binaire de classification (dendrogramme), dont la racine correspond à la classe regroupant l'ensemble des individus. Ce dendrogramme représente une hiérarchie de partitions. On peut alors choisir une partition en tronquant l'arbre à un niveau donné, le niveau dépendant soit des contraintes de l'utilisateur (l'utilisateur sait combien de classes il veut obtenir), soit de critères plus objectifs.  \nhttps://www.xlstat.com/fr/solutions/fonctionnalites/classification-ascendante-hierarchique-cah  \nhttps://perso.univ-rennes1.fr/valerie.monbet/ExposesM2/2013/Classification2.pdf  \nhttps://fr.wikipedia.org/wiki/Regroupement_hi%C3%A9rarchique  "},{"metadata":{},"cell_type":"markdown","source":"La méthode *clustermap* de seaborn parmet de visualiser un dendrogramme en 2 dimensions. Ici on utilise *rowcolors* pour visualiser la classe (espèce), qu'on associe à une couleur :"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.clustermap(X_iris, cmap=\"coolwarm\", row_colors=iris.classe.map({0:\"r\",1:\"g\",2:\"b\"}), figsize=(12,12))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Qu'observez-vous ?"},{"metadata":{},"cell_type":"markdown","source":"## MLPClassifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nfrom sklearn.neural_network import MLPClassifier\nest = MLPClassifier(hidden_layer_sizes = (20,10))\nest.max_iter = 2000\nest.fit(X_train, y_train)\ny_mlp = est.predict(X_test)\nprint(metrics.confusion_matrix(y_mlp, y_test))\nprint(metrics.classification_report(y_mlp, y_test))","execution_count":null,"outputs":[]}],"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4+"}},"nbformat":4,"nbformat_minor":1}